# -*- coding: utf-8 -*-
"""
Created on Fri Nov 10 11:09:42 2023

@author: admin
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from prettytable import PrettyTable
from PIL import Image, ImageDraw
from sklearn.metrics import roc_curve, auc, precision_recall_curve

# Define the class labels
classes = ['adware', 'backdoor', 'downloader', 'trojan', 'spyware', 'virus', 'worm']

# Initialize variables to store the results
true_positive = np.zeros(len(classes))
false_positive = np.zeros(len(classes))
false_negative = np.zeros(len(classes))
true_negative = np.zeros(len(classes))
total_rows = np.zeros(len(classes))

# Loop through each class
for i, class_label in enumerate(classes):
    # Load the CSV file into a DataFrame
    data = pd.read_csv(f'D:\\0malware_analysis\\Combine\\combined_data.csv', sep=',')

    # Define the true and predicted labels for the current class
    true_labels = [1 if label == class_label else 0 for label in data['label']]
    predicted_labels = [1 if label == class_label else 0 for label in data['predicted_label']]

    # Calculate confusion matrix for the current class
    total_rows[i] = len(true_labels)
    for j in range(len(true_labels)):
        if true_labels[j] == 1 and predicted_labels[j] == 1:
            true_positive[i] += 1
        elif true_labels[j] == 0 and predicted_labels[j] == 1:
            false_positive[i] += 1
        elif true_labels[j] == 1 and predicted_labels[j] == 0:
            false_negative[i] += 1
        else:
            true_negative[i] += 1

# Calculate metrics
precision = true_positive / (true_positive + false_positive)
recall = true_positive / (true_positive + false_negative)
f1_score = 2 * (precision * recall) / (precision + recall)
accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)

# Create a table
table = PrettyTable()
table.field_names = ["Class", "Total Rows", "True Positive", "True Negative", "False Positive", "False Negative", "Precision", "Recall", "F1 Score", "Accuracy"]

for i, class_label in enumerate(classes):
    table.add_row([
        class_label, 
        total_rows[i], 
        true_positive[i], 
        true_negative[i], 
        false_positive[i], 
        false_negative[i], 
        precision[i], 
        recall[i], 
        f1_score[i], 
        accuracy[i]
    ])

print(table)


# Save the table as an image
img = Image.new("RGB", (1000, 300), color="white")
d = ImageDraw.Draw(img)
d.text((10,10), table.get_string(), fill=(0,0,0,255))
img.save('table.png')

# Plot the metrics
x = np.arange(len(classes))

# Plot precision
plt.figure(figsize=(8, 6))
plt.bar(x, precision, color='skyblue')
plt.xlabel('Classes')
plt.ylabel('Precision')
plt.title('Precision Scores')
plt.xticks(x, classes)
plt.savefig('precision.png')
plt.show()

# Plot recall
plt.figure(figsize=(8, 6))
plt.bar(x, recall, color='lightgreen')
plt.xlabel('Classes')
plt.ylabel('Recall')
plt.title('Recall Scores')
plt.xticks(x, classes)
plt.savefig('recall.png')
plt.show()

# Plot F1 score
plt.figure(figsize=(8, 6))
plt.bar(x, f1_score, color='lightcoral')
plt.xlabel('Classes')
plt.ylabel('F1 Score')
plt.title('F1 Scores')
plt.xticks(x, classes)
plt.savefig('f1_score.png')
plt.show()

# Plot accuracy
plt.figure(figsize=(8, 6))
plt.bar(x, accuracy, color='lightblue')
plt.xlabel('Classes')
plt.ylabel('Accuracy')
plt.title('Accuracy Scores')
plt.xticks(x, classes)
plt.savefig('accuracy.png')
plt.show()


plt.figure(figsize=(12, 8))
bar_width = 0.2

plt.bar(x - 1.5 * bar_width, precision, color='skyblue', width=bar_width, label='Precision')
plt.bar(x - 0.5 * bar_width, recall, color='lightgreen', width=bar_width, label='Recall')
plt.bar(x + 0.5 * bar_width, f1_score, color='lightcoral', width=bar_width, label='F1 Score')
plt.bar(x + 1.5 * bar_width, accuracy, color='lightblue', width=bar_width, label='Accuracy')

plt.xlabel('Classes')
plt.ylabel('Scores')
plt.title('Performance Metrics by Class')
plt.xticks(x, classes)
plt.legend()
plt.savefig('performance_metrics.png')
plt.show()

# Additional plots
# Calculate ROC curve and AUC
fpr, tpr, roc_auc = {}, {}, {}
for i, class_label in enumerate(classes):
    data = pd.read_csv(f'D:\\0malware_analysis\\Combine\\combined_data.csv', sep=',')
    true_labels = [1 if label == class_label else 0 for label in data['predicted_label']]
    predicted_labels = [1 if label == class_label else 0 for label in data['label']]
    fpr[class_label], tpr[class_label], _ = roc_curve(true_labels, predicted_labels)
    roc_auc[class_label] = auc(fpr[class_label], tpr[class_label])

# Plot ROC curves
plt.figure(figsize=(8, 6))
for i, class_label in enumerate(classes):
    plt.plot(fpr[class_label], tpr[class_label], label=f'{class_label} (AUC = {roc_auc[class_label]:0.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Plot Precision-Recall curve
plt.figure(figsize=(8, 6))
for i, class_label in enumerate(classes):
    precision, recall, _ = precision_recall_curve(true_labels, predicted_labels)
    plt.plot(recall, precision, label=class_label)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='lower left')
plt.show()

