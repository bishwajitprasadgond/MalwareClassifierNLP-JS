# -*- coding: utf-8 -*-
"""
Created on Wed Feb  7 23:27:59 2024

@author: BISHU
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Assuming 'data.csv' is the file containing the provided CSV data
df = pd.read_csv('F:\Mini_malware_sample\merge_mal_ben/feature_vec_malware.csv')

# Separate features (X) and labels (y)
X = df.iloc[:, 2:]  # Starting from the 3rd column as features
y = df['family']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize classifiers
classifiers = {
    "Random Forest Classifier": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost Classifier": XGBClassifier(objective='multi:softmax', random_state=42),
    "Decision Tree Classifier": DecisionTreeClassifier(random_state=42),
    "SVM Classifier": SVC(random_state=42),
    "kNN Classifier": KNeighborsClassifier(),
    "Logistic Regression Classifier": LogisticRegression(random_state=42)
}

# Initialize dictionaries to store evaluation metrics
accuracy_scores = {}
precision_scores = {}
recall_scores = {}
roc_auc_scores = {}
fpr_dict = {}
tpr_dict = {}

# Iterate through each classifier
for clf_name, classifier in classifiers.items():
    classifier.fit(X_train, y_train)
    predictions = classifier.predict(X_test)
    
    # Calculate evaluation metrics
    accuracy_scores[clf_name] = accuracy_score(y_test, predictions)
    precision_scores[clf_name] = precision_score(y_test, predictions, average='weighted')
    recall_scores[clf_name] = recall_score(y_test, predictions, average='weighted')
    roc_auc_scores[clf_name] = roc_auc_score(y_test, classifier.predict_proba(X_test), multi_class='ovr')
    
    # Calculate ROC curve
    fpr, tpr, _ = roc_curve(pd.get_dummies(y_test), classifier.predict_proba(X_test), pos_label=1)
    fpr_dict[clf_name] = fpr
    tpr_dict[clf_name] = tpr

# Print evaluation metrics
for clf_name in classifiers.keys():
    print(clf_name + ":")
    print("Accuracy:", accuracy_scores[clf_name])
    print("Precision:", precision_scores[clf_name])
    print("Recall:", recall_scores[clf_name])
    print("ROC-AUC:", roc_auc_scores[clf_name])
    print("\n")

# Plot ROC curves
plt.figure(figsize=(10, 8))
for clf_name in classifiers.keys():
    plt.plot(fpr_dict[clf_name], tpr_dict[clf_name], label=f"{clf_name} (AUC = {roc_auc_scores[clf_name]:.2f})")

plt.plot([0, 1], [0, 1], linestyle='--', color='black')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()
