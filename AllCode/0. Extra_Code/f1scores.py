# -*- coding: utf-8 -*-
"""
Created on Fri Oct 13 14:48:14 2023

@author: bishwajit
"""



import pandas as pd
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Define the class labels
classes = ['adware', 'spyware', 'virus', 'downloader']

# Create an empty 4x4 confusion matrix
conf_matrix_total = [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]

# Create empty lists to store metrics
precision_scores = []
recall_scores = []
f1_scores = []
accuracy_scores = []

# Loop through each class
for class_label in classes:
    # Load the CSV file into a DataFrame
    data = pd.read_csv(f'G:\\new_malware_json_zip\\split_data\\similarity_matrix\\{class_label}.csv', sep=',')

    # Define the true and predicted labels for the current class
    true_labels = [class_label] * len(data)
    predicted_labels = data['label'].tolist()

    # Create a confusion matrix for the current class
    conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=classes)

    # Calculate precision, recall, F1 score, and accuracy
    precision = precision_score(true_labels, predicted_labels, average='weighted')
    recall = recall_score(true_labels, predicted_labels, average='weighted')
    f1 = f1_score(true_labels, predicted_labels, average='weighted')
    accuracy = accuracy_score(true_labels, predicted_labels)

    # Add the current class's confusion matrix to the total matrix
    conf_matrix_total += conf_matrix

    # Append metrics to lists
    precision_scores.append(precision)
    recall_scores.append(recall)
    f1_scores.append(f1)
    accuracy_scores.append(accuracy)

# Plot the total confusion matrix
sns.heatmap(conf_matrix_total, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Total Confusion Matrix')
plt.show()

# Plot precision, recall, F1 score, and accuracy
plt.figure(figsize=(10, 6))
x = range(len(classes))
plt.bar(x, precision_scores, width=0.2, label='Precision')
plt.bar([i + 0.2 for i in x], recall_scores, width=0.2, label='Recall')
plt.bar([i + 0.4 for i in x], f1_scores, width=0.2, label='F1 Score')
plt.bar([i + 0.6 for i in x], accuracy_scores, width=0.2, label='Accuracy')
plt.xticks([i + 0.3 for i in x], classes)
plt.xlabel('Classes')
plt.ylabel('Score')
plt.title('Precision, Recall, F1 Score, and Accuracy')
plt.legend()
plt.show()
