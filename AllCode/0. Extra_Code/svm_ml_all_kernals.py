# -*- coding: utf-8 -*-
"""
Created on Fri Feb  9 11:17:08 2024

@author: admin
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Feb  9 10:58:05 2024

@author: admin
"""
from sklearn.svm import SVC
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from prettytable import PrettyTable

# Load data and select only 80000 features
df = pd.read_csv('F:\Mini_malware_sample\merge_mal_ben/feature_vec_malware.csv').iloc[:, :80001]

# Separate features and labels
X = df.iloc[:, 2:]  # Starting from the 3rd column as features
y = df['family']

# Encode labels into numeric format
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize classifiers with different kernels
classifiers = {
    #"SVM Linear": SVC(kernel='linear', random_state=42),
     "SVM Polynomial Degree 3": SVC(kernel='poly', degree=3, random_state=42),
    # "SVM Polynomial Degree 4": SVC(kernel='poly', degree=4, random_state=42),
    # "SVM RBF": SVC(kernel='rbf', random_state=42),
    # "SVM Sigmoid": SVC(kernel='sigmoid', random_state=42)
}

# Initialize dictionaries to store evaluation metrics
metrics_dict = {
    "Classifier": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1 Score": [],
    "True Positive": [],
    "False Negative": [],
    "False Positive": [],
    "True Negative": []
}

# Iterate through each classifier
for clf_name, classifier in classifiers.items():
    classifier.fit(X_train, y_train)
    predictions = classifier.predict(X_test)
    
    # Calculate evaluation metrics
    # accuracy = accuracy_score(y_test, predictions)
    # precision = precision_score(y_test, predictions, average='weighted')
    # recall = recall_score(y_test, predictions, average='weighted')
    # f1 = f1_score(y_test, predictions, average='weighted')
    conf_matrix = confusion_matrix(y_test, predictions)
    


    # Loop through each class
    for i in range(1):
        # True Positive: diagonal element of the confusion matrix for class i
        true_positive = conf_matrix[i, i]
        metrics_dict["True Positive"].append(true_positive)
        
        # False Negative: sum of the elements in the ith row excluding the diagonal element
        false_negative = np.sum(conf_matrix[i, :]) - true_positive
        metrics_dict["False Negative"].append(false_negative)
        
        # False Positive: sum of the elements in the ith column excluding the diagonal element
        false_positive = np.sum(conf_matrix[:, i]) - true_positive
        metrics_dict["False Positive"].append(false_positive)
        
        # True Negative: sum of all elements in the confusion matrix excluding the ith row and ith column
        true_negative = np.sum(conf_matrix) - np.sum(conf_matrix[i, :]) - np.sum(conf_matrix[:, i]) + true_positive - false_negative - false_positive
        metrics_dict["True Negative"].append(true_negative)

        # Calculate metrics
        # Calculate precision if there are positive predictions, otherwise set it to 0
        if true_positive + false_positive != 0:
            precision = true_positive / (true_positive + false_positive)
        else:
            precision = 0
        recall = true_positive / (true_positive + false_negative)
        f1_score = 2 * (precision * recall) / (precision + recall)
        accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
    
        # Store evaluation metrics in the dictionary
        metrics_dict["Classifier"].append(clf_name)
        metrics_dict["Accuracy"].append(accuracy)
        metrics_dict["Precision"].append(precision)
        metrics_dict["Recall"].append(recall)
        metrics_dict["F1 Score"].append(f1_score)
        
        # Print confusion matrix for each classifier
        conf_matrix_table = PrettyTable()
        conf_matrix_table.title = f"Confusion Matrix for {clf_name}"
        conf_matrix_table.field_names = ["", "Predicted 0", "Predicted 1", "Predicted 2", "Predicted 3", "Predicted 4", "Predicted 5", "Predicted 6", "Predicted 7"]
        print(conf_matrix_table)
            
            
            
# Create PrettyTable for evaluation metrics
metrics_table = PrettyTable()
metrics_table.field_names = ["Classifier", "Accuracy", "Precision", "Recall", "F1 Score"]
for i in range(len(metrics_dict["Classifier"])):
    metrics_table.add_row([
        metrics_dict["Classifier"][i],
        metrics_dict["Accuracy"][i],
        metrics_dict["Precision"][i],
        metrics_dict["Recall"][i],
        metrics_dict["F1 Score"][i]
    ])

# Print evaluation metrics table
print("Evaluation Metrics:")
print(metrics_table)


