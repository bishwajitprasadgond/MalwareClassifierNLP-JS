# -*- coding: utf-8 -*-
"""
Created on Fri Nov 24 11:17:47 2023

@author: BISHWAJIT
"""

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Function for feature selection
def select_features(data):
    # You can implement your feature selection logic here
    # For demonstration, let's say we keep the top 10% of features based on variance
    threshold = 0.1
    high_variance_cols = data.var().sort_values(ascending=False).head(int(len(data.columns) * threshold)).index
    selected_features = data[high_variance_cols]
    return selected_features

# Specify the directories where your CSV files are stored
data_dirs = [
    r'C:\Users\BISHWAJIT\Documents\unigram\all_unigram_set_50k\adware',
    r'C:\Users\BISHWAJIT\Documents\unigram\all_unigram_set_50k\backdoor',
    r'C:\Users\BISHWAJIT\Documents\unigram\all_unigram_set_50k\benign',
    r'C:\Users\BISHWAJIT\Documents\unigram\all_unigram_set_50k\downloader',
    r'C:\Users\BISHWAJIT\Documents\unigram\all_unigram_set_50k\spyware',
    r'C:\Users\BISHWAJIT\Documents\unigram\all_unigram_set_50k\trojan',
    r'C:\Users\BISHWAJIT\Documents\unigram\all_unigram_set_50k\virus',
    r'C:\Users\BISHWAJIT\Documents\unigram\all_unigram_set_50k\worm'
]

# Load CSV files for each group
dfs = []
for i, data_dir in enumerate(data_dirs):
    files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
    for file in files:
        file_path = os.path.join(data_dir, file)
        df = pd.read_csv(file_path, header=None, names=[f'Group{i+1}'])
        dfs.append(df)

# Concatenate all DataFrames into one
all_data = pd.concat(dfs, axis=1)

# Perform feature selection for each group
selected_features = select_features(all_data)

# Assuming you have labels for each group (replace 'labels' with your actual labels)
labels = pd.Series([i+1 for i in range(len(data_dirs)) for _ in range(len(dfs)//len(data_dirs))])

# Normalize or standardize the data
scaler = StandardScaler()
scaled_features = scaler.fit_transform(selected_features)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(scaled_features, labels, test_size=0.2, random_state=42)

# Train a classifier (Random Forest in this example)
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
