# -*- coding: utf-8 -*-
"""
Created on Mon Jan 29 00:00:40 2024

@author: BISHU
"""
import tensorflow 
from keras import layers
from keras.models import Model

# Define the model
input = layers.Input(shape=(1024,), dtype='float32')
middle = layers.Dense(units=512, activation='relu')(input)
output = layers.Dense(units=1, activation='sigmoid')(middle)
model = Model(inputs=input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Import necessary libraries
import numpy as np
import murmur
import re
import os

# Function to read file
def read_file(sha, dir):
    with open(os.path.join(dir, sha), 'r') as fp:
        file = fp.read()
    return file

# Function to extract features
def extract_features(sha, path_to_files_dir, hash_dim=1024, split_regex=r"\s+"):
    file = read_file(sha=sha, dir=path_to_files_dir)
    tokens = re.split(pattern=split_regex, string=file)
    token_hash_buckets = [
        (murmur.string_hash(w) % (hash_dim - 1) + 1) for w in tokens
    ]
    token_bucket_counts = np.zeros(hash_dim)
    buckets, counts = np.unique(token_hash_buckets, return_counts=True)
    for bucket, count in zip(buckets, counts):
        token_bucket_counts[bucket] = count
    return np.array(token_bucket_counts)

# Training the model
model.fit(my_data, my_labels, epochs=10, batch_size=32)

# Generator function for training data
def my_generator(benign_files, malicious_files, path_to_benign_files, path_to_malicious_files, batch_size, features_length=1024):
    n_samples_per_class = batch_size // 2
    assert len(benign_files) >= n_samples_per_class
    assert len(malicious_files) >= n_samples_per_class

    while True:
        ben_features = [
            extract_features(sha, path_to_files_dir=path_to_benign_files, hash_dim=features_length)
            for sha in np.random.choice(benign_files, n_samples_per_class, replace=False)
        ]
        mal_features = [
            extract_features(sha, path_to_files_dir=path_to_malicious_files, hash_dim=features_length)
            for sha in np.random.choice(malicious_files, n_samples_per_class, replace=False)
        ]

        all_features = ben_features + mal_features
        labels = [0 for _ in range(n_samples_per_class)] + [1 for _ in range(n_samples_per_class)]
        idx = np.random.choice(range(batch_size), batch_size)
        all_features = np.array([np.array(all_features[i]) for i in idx])
        labels = np.array([labels[i] for i in idx])

        yield all_features, labels

# Setting up training data generator
batch_size = 128
features_length = 1024
path_to_training_benign_files = 'data/html/benign_files/training/'
path_to_training_malicious_files = 'data/html/malicious_files/training/'
steps_per_epoch = 1000

train_benign_files = os.listdir(path_to_training_benign_files)
train_malicious_files = os.listdir(path_to_training_malicious_files)

training_generator = my_generator(
    benign_files=train_benign_files,
    malicious_files=train_malicious_files,
    path_to_benign_files=path_to_training_benign_files,
    path_to_malicious_files=path_to_training_malicious_files,
    batch_size=batch_size,
    features_length=features_length
)

model.fit_generator(
    generator=training_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=10
)

# Validation data
path_to_validation_benign_files = 'data/html/benign_files/validation/'
path_to_validation_malicious_files = 'data/html/malicious_files/validation/'

val_benign_files = os.listdir(path_to_validation_benign_files)
val_malicious_files = os.listdir(path_to_validation_malicious_files)

validation_data = my_generator(
    benign_files=val_benign_files,
    malicious_files=val_malicious_files,
    path_to_benign_files=path_to_validation_benign_files,
    path_to_malicious_files=path_to_validation_malicious_files,
    batch_size=10000,
    features_length=features_length
).__next__()

# Model training with validation data
model.fit_generator(
    validation_data=validation_data,
    generator=training_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=10
)

# Save and load model
model.save('my_model.h5')
same_model = load_model('my_model.h5')

# ROC Curve and AUC calculation
validation_labels = validation_data[1]
validation_scores = [el[0] for el in model.predict(validation_data[0])]
fpr, tpr, thres = metrics.roc_curve(y_true=validation_labels, y_score=validation_scores)
auc = metrics.auc(fpr, tpr)
print('Validation AUC = {}'.format(auc))

# Using callbacks to save models during training
model.fit_generator(
    generator=training_generator,
    steps_per_epoch=50,
    epochs=5,
    validation_data=validation_data,
    callbacks=[
        callbacks.ModelCheckpoint(
            'results/model_epoch_{epoch}.h5',
            monitor='val_loss',
            save_best_only=False,
            save_weights_only=False
        )
    ]
)

# Custom callback to print AUC after each epoch
class MyCallback(callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        validation_labels = self.validation_data[1]
        validation_scores = self.model.predict(self.validation_data[0])
        validation_scores = [el[0] for el in validation_scores]
        fpr, tpr, thres = metrics.roc_curve(y_true=validation_labels, y_score=validation_scores)
        auc = metrics.auc(fpr, tpr)
        print('\n\tEpoch {}, Validation AUC = {}'.format(epoch, np.round(auc, 6)))

model.fit_generator(
    generator=training_generator,
    steps_per_epoch=50,
    epochs=5,
    validation_data=validation_data,
    callbacks=[
        MyCallback(),
        callbacks.ModelCheckpoint(
            'results/model_epoch_{epoch}.h5',
            monitor='val_loss',
            save_best_only=False,
            save_weights_only=False
        )
    ]
)
