# -*- coding: utf-8 -*-
"""
Created on Fri Nov  3 20:22:56 2023

@author: bishwajit
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Oct 13 14:48:14 2023

@author: bishwajit
"""

import pandas as pd
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Define the class labels
classes = ['adware', 'backdoor', 'downloader', 'trojan', 'spyware', 'virus', 'worm']

# Create an empty 7x7 confusion matrix
conf_matrix_total = [[0, 0, 0, 0, 0, 0, 0],
                     [0, 0, 0, 0, 0, 0, 0],
                     [0, 0, 0, 0, 0, 0, 0],
                     [0, 0, 0, 0, 0, 0, 0],
                     [0, 0, 0, 0, 0, 0, 0],
                     [0, 0, 0, 0, 0, 0, 0],
                     [0, 0, 0, 0, 0, 0, 0]]

# Create empty lists to store metrics
precision_scores = []
recall_scores = []
f1_scores = []
accuracy_scores = []

# Loop through each class
for class_label in classes:
    # Load the CSV file into a DataFrame
    data = pd.read_csv(f'G:\\new_malware_json_zip\\plot\\bigram_40k\\{class_label}_similarity_matrix.csv', sep=',')

    # Define the true and predicted labels for the current class
    true_labels = [class_label] * len(data)
    predicted_labels = data['label'].tolist()

    # Create a confusion matrix for the current class
    conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=classes)


    # Calculate precision, recall, F1 score, and accuracy
    precision = precision_score(true_labels, predicted_labels, average=None)
    recall = recall_score(true_labels, predicted_labels, average=None)
    f1 = f1_score(true_labels, predicted_labels, average=None)
    accuracy = accuracy_score(true_labels, predicted_labels)


    # Add the current class's confusion matrix to the total matrix
    for i in range(len(classes)):
        for j in range(len(classes)):
            conf_matrix_total[i][j] += conf_matrix[i][j]

    # Append metrics to lists
    precision_scores.append(precision)
    recall_scores.append(recall)
    f1_scores.append(f1)
    accuracy_scores.append(accuracy)

# Plot the total confusion matrix
plt.figure(figsize=(8, 6))  # Define figure size
sns.heatmap(conf_matrix_total, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Total Confusion Matrix')
plt.show()

