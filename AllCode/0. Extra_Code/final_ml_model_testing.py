# -*- coding: utf-8 -*-
"""
Created on Thu Feb  8 20:10:11 2024

@author: admin
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from prettytable import PrettyTable
import matplotlib.pyplot as plt

# Initialize dictionaries to store evaluation metrics
metrics_dict = {
    "Classifier": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1 Score": [],
    "True Positive": [],
    "False Negative": [],
    "False Positive": [],
    "True Negative": []
}

# Load data and select only 80000 features
df = pd.read_csv('F:\Mini_malware_sample\merge_mal_ben/feature_vec_malware.csv').iloc[:, :80001]

# Separate features and labels
X = df.iloc[:, 2:]  # Starting from the 3rd column as features
y = df['family']

# Encode labels into numeric format
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train= np.array(X_train)
# Initialize classifiers
classifiers = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(objective='multi:softmax', random_state=42),
    "LightGBM": LGBMClassifier(random_state=42),
    "k-Nearest Neighbors": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "SVM": SVC(random_state=42)
}

# Iterate through each classifier
for clf_name, classifier in classifiers.items():
    classifier.fit(X_train, y_train)
    predictions = classifier.predict(X_test)
    
    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, average='weighted')
    recall = recall_score(y_test, predictions, average='weighted')
    f1 = f1_score(y_test, predictions, average='weighted')
    conf_matrix = confusion_matrix(y_test, predictions)
    
    # Handle confusion matrix based on its shape
    if conf_matrix.shape == (2, 2):  # Binary classification
        tn, fp, fn, tp = conf_matrix.ravel()
    else:  # Multi-class classification
        tp, fn, fp, tn = conf_matrix.ravel()
    
    # Store metrics in the dictionary
    metrics_dict["Classifier"].append(clf_name)
    metrics_dict["Accuracy"].append(accuracy)
    metrics_dict["Precision"].append(precision)
    metrics_dict["Recall"].append(recall)
    metrics_dict["F1 Score"].append(f1)
    metrics_dict["True Positive"].append(tp)
    metrics_dict["False Negative"].append(fn)
    metrics_dict["False Positive"].append(fp)
    metrics_dict["True Negative"].append(tn)

# Create a DataFrame from the dictionary
metrics_df = pd.DataFrame(metrics_dict)

# Save DataFrame to a CSV file
metrics_df.to_csv("metrics.csv", index=False)

# Print the DataFrame
print(metrics_df)

# Create a PrettyTable
table = PrettyTable()
table.field_names = ["Classifier", "Accuracy", "Precision", "Recall", "F1 Score", "True Positive", "False Negative", "False Positive", "True Negative"]

for index, row in metrics_df.iterrows():
    table.add_row([row["Classifier"], row["Accuracy"], row["Precision"], row["Recall"], row["F1 Score"], row["True Positive"], row["False Negative"], row["False Positive"], row["True Negative"]])

print(table)

# Plotting accuracy, precision, recall, and F1-score
plt.figure(figsize=(12, 6))

# Accuracy
plt.subplot(2, 2, 1)
plt.bar(metrics_df["Classifier"], metrics_df["Accuracy"], color='blue')
plt.title('Accuracy')
plt.ylabel('Score')

# Precision
plt.subplot(2, 2, 2)
plt.bar(metrics_df["Classifier"], metrics_df["Precision"], color='green')
plt.title('Precision')
plt.ylabel('Score')

# Recall
plt.subplot(2, 2, 3)
plt.bar(metrics_df["Classifier"], metrics_df["Recall"], color='orange')
plt.title('Recall')
plt.ylabel('Score')

# F1 Score
plt.subplot(2, 2, 4)
plt.bar(metrics_df["Classifier"], metrics_df["F1 Score"], color='red')
plt.title('F1 Score')
plt.ylabel('Score')

plt.tight_layout()
plt.show()
