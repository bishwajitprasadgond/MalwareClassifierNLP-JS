# -*- coding: utf-8 -*-
"""
Created on Mon Dec 18 19:20:58 2023

@author: BISHWAJIT
"""
import numpy as np
import murmur
import re
import os
from keras import layers
from keras.models import Model

# Assuming wshape is the input shape, correct it to 'shape'
input_layer = layers.Input(shape=(1024,), dtype='float32')
middle = layers.Dense(units=512, activation='relu')(input_layer)
output = layers.Dense(units=1, activation='sigmoid')(middle)

model = Model(inputs=input_layer, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

####################################################################################
'''shows the feature extraction code'''
def read_file(sha, dir):
    # Read the content of the file with the given SHA from the specified directory
    with open(os.path.join(dir, sha), 'r') as fp:
        file_content = fp.read()
    return file_content

def extract_features(sha, path_to_files_dir, hash_dim=1024, split_regex=r"\s+"):
    # Read the file content
    file_content = read_file(sha=sha, dir=path_to_files_dir)
    
    # Tokenize the file content using the specified split_regex
    tokens = re.split(pattern=split_regex, string=file_content)

    # Hash each token and replace it by a bucket (category) from 1:hash_dim
    token_hash_buckets = [
        (murmur.string_hash(w) % (hash_dim - 1) + 1) for w in tokens
    ]

    # Count how many hits each bucket got, ensuring features always have length hash_dim
    token_bucket_counts = np.zeros(hash_dim)

    # Get the frequency counts for each unique value in token_hash_buckets
    buckets, counts = np.unique(token_hash_buckets, return_counts=True)

    # Insert the counts into the token_bucket_counts object
    for bucket, count in zip(buckets, counts):
        token_bucket_counts[bucket] = count

    return np.array(token_bucket_counts)

# Example usage:
# features = extract_features(sha='example_sha', path_to_files_dir='example_dir')

######################################################################################
'''shows how we can create our own data generator using our feature extraction function'''
def my_generator(benign_files, malicious_files,
                 path_to_benign_files, path_to_malicious_files,
                 batch_size, features_length=1024):
    n_samples_per_class = batch_size // 2
    assert len(benign_files) >= n_samples_per_class
    assert len(malicious_files) >= n_samples_per_class

    while True:
        ben_features = [
            extract_features(sha, path_to_files_dir=path_to_benign_files,
                             hash_dim=features_length)
            for sha in np.random.choice(benign_files, n_samples_per_class, replace=False)
        ]

        mal_features = [
            extract_features(sha, path_to_files_dir=path_to_malicious_files,
                             hash_dim=features_length)
            for sha in np.random.choice(malicious_files, n_samples_per_class, replace=False)
        ]

        all_features = ben_features + mal_features
        labels = [0 for _ in range(n_samples_per_class)] + [1 for _ in range(n_samples_per_class)]

        idx = np.random.choice(range(batch_size), batch_size)
        all_features = np.array([np.array(all_features[i]) for i in idx])
        labels = np.array([labels[i] for i in idx])

        yield all_features, labels

#################################################################################
''' shows how to create a training data generator, and how to train our model by passing the 
generator to our modelâ€™s fit_generator method.
'''
batch_size = 128
features_length = 1024
path_to_training_benign_files = 'data/html/benign_files/training/'
path_to_training_malicious_files = 'data/html/malicious_files/training/'
steps_per_epoch = 1000  # Adjust this based on your dataset size and batch size

train_benign_files = os.listdir(path_to_training_benign_files)
train_malicious_files = os.listdir(path_to_training_malicious_files)

# Make our training data generator
training_generator = my_generator(
    benign_files=train_benign_files,
    malicious_files=train_malicious_files,
    path_to_benign_files=path_to_training_benign_files,
    path_to_malicious_files=path_to_training_malicious_files,
    batch_size=batch_size,
    features_length=features_length
)

# Use model.fit() instead of fit_generator for recent versions of Keras
model.fit(
    x=training_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=10
)

#################################################################################
'''shows how to load our validation features into memory using our my_generator function'''


path_to_validation_benign_files = 'data/html/benign_files/validation/'
path_to_validation_malicious_files = 'data/html/malicious_files/validation/'

# get the validation keys
val_benign_file_keys = os.listdir(path_to_validation_benign_files)
val_malicious_file_keys = os.listdir(path_to_validation_malicious_files)

# grab the validation data and extract the features
validation_data_generator = my_generator(
    benign_files=val_benign_file_keys,
    malicious_files=val_malicious_file_keys,
    path_to_benign_files=path_to_validation_benign_files,
    path_to_malicious_files=path_to_validation_malicious_files,
    batch_size=10000,
    features_length=features_length
)

# Get the validation data using the generator
validation_data = next(validation_data_generator)
####################################################################

model.fit_generator(
 validation_data=validation_data,
 generator=training_generator,
 steps_per_epoch=steps_per_epoch,
 epochs=10
)