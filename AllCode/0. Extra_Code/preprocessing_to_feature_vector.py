# -*- coding: utf-8 -*-
"""
Created on Tue Feb  6 19:38:10 2024

@author: BISHWAJIT
"""

import json
import time
import re
import os
import pandas as pd
from tqdm import tqdm

#%%
# Specify the directory path

path = 'F:\Malware_old\Downloader\JSON'

# Specify the location where you want to save the output files
output_path = path + r'\cat'  # Change this to your desired output directory
os.makedirs(output_path, exist_ok=True)
# Create four output directories if they don't exist
output_categories = ['api_name', 'api_arguments', 'api_return', 'api_cat']
for category in output_categories:
    category_path = os.path.join(output_path, category)
    os.makedirs(category_path, exist_ok=True)

# Create separate progress files for each category
progress_files = {}
for category in output_categories:
    progress_files[category] = os.path.join(output_path, category, "progress.txt")

# Initialize variables to keep track of the last successfully processed file for each category
last_successful_file_indices = {category: -1 for category in output_categories}

# Load progress for each category from their respective progress files
for category, progress_file_path in progress_files.items():
    if os.path.exists(progress_file_path):
        with open(progress_file_path, 'r') as progress_file:
            last_successful_file_indices[category] = int(progress_file.read())

# List all files in the specified directory
dirs = os.listdir(path)

for a in range(last_successful_file_indices['api_name'] + 1, len(dirs)):
    print(a)
    filetoopen = os.path.join(path, dirs[a])  # Use os.path.join to create a valid file path

    try:
        with open(filetoopen, 'r', encoding='utf-8') as json_file:
            data = json.load(json_file)

        api_name = []
        api_arguments = []
        api_return = []
        api_cat = []

        if 'behavior' in data and 'processes' in data['behavior']:
            sign = data['behavior']['processes']

            for k in range(0, len(sign)):
                api_marks = sign[k]['calls']
                if len(api_marks) > 0:
                    for x in range(0, len(api_marks)):
                        intrs = api_marks[x]
                        api_name.append(intrs['api'])

                        # Convert arguments to a string and remove spaces
                        arguments_str = ",".join(str(value) for value in intrs['arguments'].values())
                        arguments_str = re.sub(r'\s', '', arguments_str)
                        api_arguments.append(arguments_str.encode('utf-8', 'replace').decode())

                        # Convert return value to a string
                        api_return.append(str(intrs['return_value']))
                        api_cat.append(intrs['category'])
        else:
            print(f"Key 'behavior' or 'processes' not found in {filetoopen}. Skipping.")

        # Handle 'apistats' if available
        if 'behavior' in data and 'apistats' in data['behavior']:
            behav = data['behavior']['apistats']
            key_list = list(behav.keys())
            for p in range(0, len(key_list)):
                extra_ent = behav[key_list[p]]
                sequence = [key for key, value in extra_ent.items() for _ in range(value)]
                api_name.extend(sequence)

                # Calculate differences and fill with 'na'
                len_api_name = len(api_name)
                diff_name_arg = len_api_name - len(api_arguments)
                my_list_name_arg = ["na"] * diff_name_arg
                diff_name_ret = len_api_name - len(api_return)
                my_list_name_ret = ["na"] * diff_name_ret
                diff_name_cat = len_api_name - len(api_cat)
                my_list_name_cat = ["na"] * diff_name_cat

                api_arguments.extend(my_list_name_arg)
                api_return.extend(my_list_name_ret)
                api_cat.extend(my_list_name_cat)
        else:
            print(f"Key 'apistats' not found in {filetoopen}. Skipping.")

        file_path = data['target']['file']['md5']

        # Specify the full path for saving the output files in their respective folders
        name = os.path.join(output_path, 'api_name', "api_" + file_path + ".txt")
        argp = os.path.join(output_path, 'api_arguments', "arg_" + file_path + ".txt")
        retp = os.path.join(output_path, 'api_return', "ret_" + file_path + ".txt")
        catp = os.path.join(output_path, 'api_cat', "cat_" + file_path + ".txt")

        # Write the lists to files
        with open(name, 'w', encoding='utf-8') as file:
            for item in api_name:
                file.write(item + '\n')

        print(f"The list has been written to '{name}'.")

        with open(argp, 'w', encoding='utf-8') as file:
            for item in api_arguments:
                file.write(item + '\n')

        print(f"The list has been written to '{argp}'.")

        with open(retp, 'w', encoding='utf-8') as file:
            for item in api_return:
                file.write(item + '\n')

        print(f"The list has been written to '{retp}'.")

        with open(catp, 'w', encoding='utf-8') as file:
            for item in api_cat:
                file.write(item + '\n')

        print(f"The list has been written to '{catp}'.")

        # Update the progress file for each category with the last successfully processed file index
        for category, progress_file_path in progress_files.items():
            with open(progress_file_path, 'w') as progress_file:
                progress_file.write(str(a))

    except Exception as e:
        print(f"Error processing {filetoopen}: {e}")

# Remove the progress files if all files have been processed successfully
for category, progress_file_path in progress_files.items():
    if os.path.exists(progress_file_path):
        os.remove(progress_file_path)



#%%

path = 'F:\Malware_old\Downloader'

# Specify the directories containing the source files
source_directory =  path + r'\cat\api_name'  # Update with your source directory path
arg_directory =  path + r'\cat\api_arguments'  # Update with your argument directory path

# Specify the output directory
output_directory = path + r'\cat\api_name_api_arguments'  # Update with your desired output directory path
os.makedirs(output_directory, exist_ok=True)

# List all files in the source directory
source_files = os.listdir(source_directory)

# Iterate through the source files
for source_file_name in source_files:
    # Construct the full file paths
    source_file_path = os.path.join(source_directory, source_file_name)
    arg_file_name = source_file_name.replace("api_", "arg_")
    arg_file_path = os.path.join(arg_directory, arg_file_name)

    # Check if the corresponding argument file exists
    if not os.path.isfile(arg_file_path):
        print(f"Argument file not found for {source_file_name}. Skipping.")
        continue

    # Read data from both source and argument files
    with open(source_file_path, 'r', encoding='utf-8') as source_file, \
         open(arg_file_path, 'r', encoding='utf-8') as arg_file:

        source_lines = source_file.readlines()
        arg_lines = arg_file.readlines()

        # Combine lines from api_name and api_arguments files
        merged_lines = [f"{source.strip()},{arg.strip()}\n" for source, arg in zip(source_lines, arg_lines)]

        # Construct the merged file name
        merged_file_name = f"api_name_api_argument_{source_file_name}"

        # Construct the full output file path
        output_file_path = os.path.join(output_directory, merged_file_name)

        # Write the merged lines to the output file
        with open(output_file_path, 'w', encoding='utf-8') as output_file:
            output_file.writelines(merged_lines)

        print(f"Merged file '{merged_file_name}' saved in '{output_directory}'.")

print("Merging and renaming completed.")



#%% Removing hexadecimal addresses and replace commas with underscores


# Function to remove hexadecimal addresses and replace commas with underscores
def clean_line(line):
    # Split the line into parts using commas
    parts = line.split(',')
    
    # Determine the size of the split
    split_size = len(parts)

    # Remove any parts starting with "0x" from the split
    if split_size >= 1:

        #cleaned_parts = [part for part in parts if not part.startswith("0x")]
        cleaned_parts = [part for part in parts if not re.match(r'^[0-9]+$', part) and not re.match(r'^[!@#$%^&*(),.?":{}|<>]+$', part) and not part.startswith("0x")]
    else:
        cleaned_parts = parts
    
    # Replace commas with underscores in the cleaned parts
    cleaned_line = '_'.join(cleaned_parts)
    
    return cleaned_line

# Input folder path containing text files
input_folder = path + r'\cat\api_name_api_arguments'  # Replace with your input folder path

# Output folder path to store cleaned files
output_folder = path + r'\cat\api_name_arguments_clean'  # Replace with your output folder path
os.makedirs(output_folder, exist_ok=True)

# Ensure the output folder exists or create it if it doesn't
if not os.path.exists(output_folder):
    os.makedirs(output_folder)

# Loop through all files in the input folder
for filename in os.listdir(input_folder):
    if filename.endswith('.txt'):
        input_file_path = os.path.join(input_folder, filename)
        output_file_path = os.path.join(output_folder, filename)

        # Open the input file for reading with 'utf-8' encoding
        with open(input_file_path, 'r', encoding='utf-8') as input_file, open(output_file_path, 'w', encoding='utf-8') as output_file:
            # Iterate through the lines in the input file
            for line in input_file:
                # Clean the line and write it to the output file
                cleaned_line = clean_line(line)
                if cleaned_line:
                    output_file.write(cleaned_line + '\n')

        print(f"Processing complete for '{filename}'. Cleaned data saved in '{output_file_path}'.")



#%% Cleaning Space of Cleaned API name and argument


# Function to remove blank lines from a file
def remove_blank_lines(input_file_path, output_file_path):
    # Read the file and remove blank lines
    with open(input_file_path, 'r', encoding='utf-8') as input_file:
        lines = input_file.readlines()
    
    # Write the non-blank lines to the output file
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        for line in lines:
            if line.strip():
                output_file.write(line)

# Specify the folder containing the input files
input_folder = path + r'\cat\api_name_arguments_clean'  # Replace with the path to your input folder

# Specify the folder to save the output files
output_folder =  path + r'\cat\api_name_arguments_clean_space'  # Replace with the path to your output folder
os.makedirs(output_folder, exist_ok=True)

# Check if the input folder exists
if os.path.exists(input_folder):
    # Create the output folder if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Iterate through the files in the input folder
    for filename in os.listdir(input_folder):
        input_file_path = os.path.join(input_folder, filename)
        
        # Check if it's a file
        if os.path.isfile(input_file_path):
            print(f"Processing '{filename}'...")
            
            # Construct the output file path in the output folder
            output_file_path = os.path.join(output_folder, filename)
            
            # Remove blank lines from the input file and save to the output file
            remove_blank_lines(input_file_path, output_file_path)
            
            print(f"Blank lines removed from '{filename}' and saved to '{output_file_path}'.")
else:
    print(f"The input folder '{input_folder}' does not exist.")



#%%  Making Unigram of Each file



# Input folder path containing text files
input_folder = path + r'\cat\api_name_arguments_clean_space'  # Replace with the path to your input folder

# Output folder path to store unigram files in CSV format
output_folder = path + r'\cat\unigram_created'  # Replace with the path to your output folder
os.makedirs(output_folder, exist_ok=True)

# Check if the input folder exists
if os.path.exists(input_folder):
    # Create the output folder if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # Get the list of files in the input folder
    files = [filename for filename in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, filename))]

    # Set up the tqdm progress bar
    progress_bar = tqdm(total=len(files), desc='Processing Files', unit='file')

    # Iterate through the files in the input folder
    for filename in files:
        input_file_path = os.path.join(input_folder, filename)

        # Read the text from the input file
        with open(input_file_path, 'r', encoding='utf-8') as input_file:
            text = input_file.read()

        # Tokenize the text at line breaks and avoid splitting at special characters
        unigrams = re.split(r'[\r\n]+', text)

        # Remove empty lines
        unigrams = [line.strip() for line in unigrams if line.strip()]

        # Remove the .txt extension from the filename
        filename_without_extension = os.path.splitext(filename)[0]

        # Construct the output file path in the output folder
        output_csv_file_path = os.path.join(output_folder, f'{filename_without_extension}.csv')

        # Create a DataFrame from the list of unigrams
        df = pd.DataFrame({'Unigram': unigrams})

        # Save the unigrams to a CSV file with an escape character
        df.to_csv(output_csv_file_path, index=False, encoding='utf-8', escapechar='\\')

        # Update the progress bar
        progress_bar.update(1)
        progress_bar.set_postfix(File=filename)

    # Close the progress bar
    progress_bar.close()

    print("Processing completed.")

else:
    print(f"The input folder '{input_folder}' does not exist.")



#%% Making Unique Unigram


#name = 'benign'

# Input folder path containing CSV files
#input_folder = r'C:\sample\mal_classes\\' + name + '_unigram_created'
input_folder = path + r'\cat\unigram_created'
# Output file path to store the merged unique rows
#output_csv_file = r'C:\sample\mal_classes\\' + name +'.csv'
output_csv_file = path + r'\cat\benign.csv'
# Initialize an empty DataFrame to store unique rows
unique_rows_df = pd.DataFrame()

# Check if the input folder exists
if os.path.exists(input_folder):
    # Get the list of CSV files in the input folder
    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]
    total_files = len(csv_files)

    # Create a progress bar
    progress_bar = tqdm(total=total_files, unit=' file(s)')

    # Iterate through the CSV files in the input folder
    for filename in csv_files:
        csv_file_path = os.path.join(input_folder, filename)

        # Read the CSV file into a DataFrame
        df = pd.read_csv(csv_file_path)

        # Append the unique rows to the DataFrame
        unique_rows_df = pd.concat([unique_rows_df, df]).drop_duplicates().reset_index(drop=True)

        # Update the progress bar
        progress_bar.update(1)
        
        # Calculate the percentage completion
        percentage_complete = (progress_bar.n / total_files) * 100

        # Print the percentage completion every 30 seconds
        if progress_bar.n % 5 == 0:
            print(f"{percentage_complete:.2f}% completed")

        # Sleep for 30 seconds
        time.sleep(30)

    # Close the progress bar
    progress_bar.close()

    # Save the merged unique rows to a new CSV file
    unique_rows_df.to_csv(output_csv_file, index=False)

    print(f"Unique rows merged and saved in '{output_csv_file}'.")
else:
    print(f"The input folder '{input_folder}' does not exist.")



#%%

# # -*- coding: utf-8 -*-
# """
# Created on Tue Feb  6 15:34:47 2024

# @author: BISHWAJIT
# """



# # Read the CSV file into a DataFrame
# df = pd.read_csv(r'C:\Unique_Unigram\benign.csv')

# # Filter rows based on the allowed function prefixes and skip rows starting with "_"
# filtered_df = df[df['Unigram'].apply(lambda x: not x.startswith('_'))]  # Added closing parenthesis

# # Save the new DataFrame to a new CSV file
# filtered_df.to_csv(r'C:\Unique_Unigram\filtered_benign.csv', index=False)



#%%