{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ead1866",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e32c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Created on Sat Oct  7 14:29:53 2023\n",
    "@author: bishwajit\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32141e80",
   "metadata": {},
   "source": [
    "# JSON to 4 Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65455a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "\n",
    "path = 'F:\\Malware_old\\malware_dataset\\malware_samples\\Trojan'\n",
    "\n",
    "# Specify the location where you want to save the output files\n",
    "output_path = path + r'\\cat'  # Change this to your desired output directory\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "# Create four output directories if they don't exist\n",
    "output_categories = ['api_name', 'api_arguments', 'api_return', 'api_cat']\n",
    "for category in output_categories:\n",
    "    category_path = os.path.join(output_path, category)\n",
    "    os.makedirs(category_path, exist_ok=True)\n",
    "\n",
    "# Create separate progress files for each category\n",
    "progress_files = {}\n",
    "for category in output_categories:\n",
    "    progress_files[category] = os.path.join(output_path, category, \"progress.txt\")\n",
    "\n",
    "# Initialize variables to keep track of the last successfully processed file for each category\n",
    "last_successful_file_indices = {category: -1 for category in output_categories}\n",
    "\n",
    "# Load progress for each category from their respective progress files\n",
    "for category, progress_file_path in progress_files.items():\n",
    "    if os.path.exists(progress_file_path):\n",
    "        with open(progress_file_path, 'r') as progress_file:\n",
    "            last_successful_file_indices[category] = int(progress_file.read())\n",
    "\n",
    "# List all files in the specified directory\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "for a in range(last_successful_file_indices['api_name'] + 1, len(dirs)):\n",
    "    print(a)\n",
    "    filetoopen = os.path.join(path, dirs[a])  # Use os.path.join to create a valid file path\n",
    "\n",
    "    try:\n",
    "        with open(filetoopen, 'r', encoding='utf-8') as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        api_name = []\n",
    "        api_arguments = []\n",
    "        api_return = []\n",
    "        api_cat = []\n",
    "\n",
    "        if 'behavior' in data and 'processes' in data['behavior']:\n",
    "            sign = data['behavior']['processes']\n",
    "\n",
    "            for k in range(0, len(sign)):\n",
    "                api_marks = sign[k]['calls']\n",
    "                if len(api_marks) > 0:\n",
    "                    for x in range(0, len(api_marks)):\n",
    "                        intrs = api_marks[x]\n",
    "                        api_name.append(intrs['api'])\n",
    "\n",
    "                        # Convert arguments to a string and remove spaces\n",
    "                        arguments_str = \",\".join(str(value) for value in intrs['arguments'].values())\n",
    "                        arguments_str = re.sub(r'\\s', '', arguments_str)\n",
    "                        api_arguments.append(arguments_str.encode('utf-8', 'replace').decode())\n",
    "\n",
    "                        # Convert return value to a string\n",
    "                        api_return.append(str(intrs['return_value']))\n",
    "                        api_cat.append(intrs['category'])\n",
    "        else:\n",
    "            print(f\"Key 'behavior' or 'processes' not found in {filetoopen}. Skipping.\")\n",
    "\n",
    "        # Handle 'apistats' if available\n",
    "        if 'behavior' in data and 'apistats' in data['behavior']:\n",
    "            behav = data['behavior']['apistats']\n",
    "            key_list = list(behav.keys())\n",
    "            for p in range(0, len(key_list)):\n",
    "                extra_ent = behav[key_list[p]]\n",
    "                sequence = [key for key, value in extra_ent.items() for _ in range(value)]\n",
    "                api_name.extend(sequence)\n",
    "\n",
    "                # Calculate differences and fill with 'na'\n",
    "                len_api_name = len(api_name)\n",
    "                diff_name_arg = len_api_name - len(api_arguments)\n",
    "                my_list_name_arg = [\"na\"] * diff_name_arg\n",
    "                diff_name_ret = len_api_name - len(api_return)\n",
    "                my_list_name_ret = [\"na\"] * diff_name_ret\n",
    "                diff_name_cat = len_api_name - len(api_cat)\n",
    "                my_list_name_cat = [\"na\"] * diff_name_cat\n",
    "\n",
    "                api_arguments.extend(my_list_name_arg)\n",
    "                api_return.extend(my_list_name_ret)\n",
    "                api_cat.extend(my_list_name_cat)\n",
    "        else:\n",
    "            print(f\"Key 'apistats' not found in {filetoopen}. Skipping.\")\n",
    "\n",
    "        file_path = data['target']['file']['md5']\n",
    "\n",
    "        # Specify the full path for saving the output files in their respective folders\n",
    "        name = os.path.join(output_path, 'api_name', \"api_\" + file_path + \".txt\")\n",
    "        argp = os.path.join(output_path, 'api_arguments', \"arg_\" + file_path + \".txt\")\n",
    "        retp = os.path.join(output_path, 'api_return', \"ret_\" + file_path + \".txt\")\n",
    "        catp = os.path.join(output_path, 'api_cat', \"cat_\" + file_path + \".txt\")\n",
    "\n",
    "        # Write the lists to files\n",
    "        with open(name, 'w', encoding='utf-8') as file:\n",
    "            for item in api_name:\n",
    "                file.write(item + '\\n')\n",
    "\n",
    "        print(f\"The list has been written to '{name}'.\")\n",
    "\n",
    "        with open(argp, 'w', encoding='utf-8') as file:\n",
    "            for item in api_arguments:\n",
    "                file.write(item + '\\n')\n",
    "\n",
    "        print(f\"The list has been written to '{argp}'.\")\n",
    "\n",
    "        with open(retp, 'w', encoding='utf-8') as file:\n",
    "            for item in api_return:\n",
    "                file.write(item + '\\n')\n",
    "\n",
    "        print(f\"The list has been written to '{retp}'.\")\n",
    "\n",
    "        with open(catp, 'w', encoding='utf-8') as file:\n",
    "            for item in api_cat:\n",
    "                file.write(item + '\\n')\n",
    "\n",
    "        print(f\"The list has been written to '{catp}'.\")\n",
    "\n",
    "        # Update the progress file for each category with the last successfully processed file index\n",
    "        for category, progress_file_path in progress_files.items():\n",
    "            with open(progress_file_path, 'w') as progress_file:\n",
    "                progress_file.write(str(a))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filetoopen}: {e}\")\n",
    "\n",
    "# Remove the progress files if all files have been processed successfully\n",
    "for category, progress_file_path in progress_files.items():\n",
    "    if os.path.exists(progress_file_path):\n",
    "        os.remove(progress_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Specify the directories containing the source files\n",
    "source_directory =  path + r'\\cat\\api_name'  # Update with your source directory path\n",
    "arg_directory =  path + r'\\cat\\api_arguments'  # Update with your argument directory path\n",
    "\n",
    "# Specify the output directory\n",
    "output_directory = r'F:\\Malware_old\\malware_dataset\\malware_samples\\Trojan\\cat\\api_name_arguments'  # Update with your desired output directory path\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# List all files in the source directory\n",
    "source_files = os.listdir(source_directory)\n",
    "\n",
    "# Iterate through the source files\n",
    "for source_file_name in source_files:\n",
    "    # Construct the full file paths\n",
    "    source_file_path = os.path.join(source_directory, source_file_name)\n",
    "    arg_file_name = source_file_name.replace(\"api_\", \"arg_\")\n",
    "    arg_file_path = os.path.join(arg_directory, arg_file_name)\n",
    "\n",
    "    # Check if the corresponding argument file exists\n",
    "    if not os.path.isfile(arg_file_path):\n",
    "        print(f\"Argument file not found for {source_file_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Read data from both source and argument files\n",
    "    with open(source_file_path, 'r', encoding='utf-8') as source_file, \\\n",
    "         open(arg_file_path, 'r', encoding='utf-8') as arg_file:\n",
    "\n",
    "        source_lines = source_file.readlines()\n",
    "        arg_lines = arg_file.readlines()\n",
    "\n",
    "        # Combine lines from api_name and api_arguments files\n",
    "        merged_lines = [f\"{source.strip()},{arg.strip()}\\n\" for source, arg in zip(source_lines, arg_lines)]\n",
    "\n",
    "        # Construct the merged file name\n",
    "        merged_file_name = f\"api_name_api_argument_{source_file_name}\"\n",
    "\n",
    "        # Construct the full output file path\n",
    "        output_file_path = os.path.join(output_directory, merged_file_name)\n",
    "\n",
    "        # Write the merged lines to the output file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.writelines(merged_lines)\n",
    "\n",
    "        print(f\"Merged file '{merged_file_name}' saved in '{output_directory}'.\")\n",
    "\n",
    "print(\"Merging and renaming completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to remove hexadecimal addresses and replace commas with underscores\n",
    "def clean_line(line):\n",
    "    # Split the line into parts using commas\n",
    "    parts = line.split(',')\n",
    "    \n",
    "    # Determine the size of the split\n",
    "    split_size = len(parts)\n",
    "\n",
    "    # Remove any parts starting with \"0x\" from the split\n",
    "    if split_size >= 1:\n",
    "\n",
    "        #cleaned_parts = [part for part in parts if not part.startswith(\"0x\")]\n",
    "        cleaned_parts = [part for part in parts if not re.match(r'^[0-9]+$', part) and not re.match(r'^[!@#$%^&*(),.?\":{}|<>]+$', part) and not part.startswith(\"0x\")]\n",
    "    else:\n",
    "        cleaned_parts = parts\n",
    "    \n",
    "    # Replace commas with underscores in the cleaned parts\n",
    "    cleaned_line = '_'.join(cleaned_parts)\n",
    "    \n",
    "    return cleaned_line\n",
    "\n",
    "# Input folder path containing text files\n",
    "input_folder = path + r'\\cat\\api_arguments'  # Replace with your input folder path\n",
    "\n",
    "# Output folder path to store cleaned files\n",
    "output_folder = path + r'\\cat\\api_name_arguments_clean'  # Replace with your output folder path\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Ensure the output folder exists or create it if it doesn't\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Loop through all files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_file_path = os.path.join(input_folder, filename)\n",
    "        output_file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Open the input file for reading with 'utf-8' encoding\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as input_file, open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            # Iterate through the lines in the input file\n",
    "            for line in input_file:\n",
    "                # Clean the line and write it to the output file\n",
    "                cleaned_line = clean_line(line)\n",
    "                if cleaned_line:\n",
    "                    output_file.write(cleaned_line + '\\n')\n",
    "\n",
    "        print(f\"Processing complete for '{filename}'. Cleaned data saved in '{output_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161300bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to remove blank lines from a file\n",
    "def remove_blank_lines(input_file_path, output_file_path):\n",
    "    # Read the file and remove blank lines\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        lines = input_file.readlines()\n",
    "    \n",
    "    # Write the non-blank lines to the output file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                output_file.write(line)\n",
    "\n",
    "# Specify the folder containing the input files\n",
    "input_folder = path + r'\\cat\\api_name_arguments_clean'  # Replace with the path to your input folder\n",
    "\n",
    "# Specify the folder to save the output files\n",
    "output_folder =  path + r'\\cat\\api_name_arguments_clean'  # Replace with the path to your output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Check if the input folder exists\n",
    "if os.path.exists(input_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate through the files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Check if it's a file\n",
    "        if os.path.isfile(input_file_path):\n",
    "            print(f\"Processing '{filename}'...\")\n",
    "            \n",
    "            # Construct the output file path in the output folder\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # Remove blank lines from the input file and save to the output file\n",
    "            remove_blank_lines(input_file_path, output_file_path)\n",
    "            \n",
    "            print(f\"Blank lines removed from '{filename}' and saved to '{output_file_path}'.\")\n",
    "else:\n",
    "    print(f\"The input folder '{input_folder}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Input folder path containing text files\n",
    "input_folder = path + r'\\cat\\api_name_arguments_clean_space'  # Replace with the path to your input folder\n",
    "\n",
    "# Output folder path to store unigram files in CSV format\n",
    "output_folder = path + r'\\cat\\trojan_unigram_created'  # Replace with the path to your output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Check if the input folder exists\n",
    "if os.path.exists(input_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Get the list of files in the input folder\n",
    "    files = [filename for filename in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, filename))]\n",
    "\n",
    "    # Set up the tqdm progress bar\n",
    "    progress_bar = tqdm(total=len(files), desc='Processing Files', unit='file')\n",
    "\n",
    "    # Iterate through the files in the input folder\n",
    "    for filename in files:\n",
    "        input_file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Read the text from the input file\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "            text = input_file.read()\n",
    "\n",
    "        # Tokenize the text at line breaks and avoid splitting at special characters\n",
    "        unigrams = re.split(r'[\\r\\n]+', text)\n",
    "\n",
    "        # Remove empty lines\n",
    "        unigrams = [line.strip() for line in unigrams if line.strip()]\n",
    "\n",
    "        # Remove the .txt extension from the filename\n",
    "        filename_without_extension = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Construct the output file path in the output folder\n",
    "        output_csv_file_path = os.path.join(output_folder, f'{filename_without_extension}.csv')\n",
    "\n",
    "        # Create a DataFrame from the list of unigrams\n",
    "        df = pd.DataFrame({'Unigram': unigrams})\n",
    "\n",
    "        # Save the unigrams to a CSV file with an escape character\n",
    "        df.to_csv(output_csv_file_path, index=False, encoding='utf-8', escapechar='\\\\')\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(File=filename)\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    print(\"Processing completed.\")\n",
    "\n",
    "else:\n",
    "    print(f\"The input folder '{input_folder}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c2e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = 'benign'\n",
    "\n",
    "# Input folder path containing CSV files\n",
    "#input_folder = r'C:\\sample\\mal_classes\\\\' + name + '_unigram_created'\n",
    "input_folder = path + r'\\cat\\trojan_unigram_created'\n",
    "# Output file path to store the merged unique rows\n",
    "#output_csv_file = r'C:\\sample\\mal_classes\\\\' + name +'.csv'\n",
    "output_csv_file = path + r'\\cat\\Trojan.csv'\n",
    "# Initialize an empty DataFrame to store unique rows\n",
    "unique_rows_df = pd.DataFrame()\n",
    "\n",
    "# Check if the input folder exists\n",
    "if os.path.exists(input_folder):\n",
    "    # Get the list of CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    total_files = len(csv_files)\n",
    "\n",
    "    # Create a progress bar\n",
    "    progress_bar = tqdm(total=total_files, unit=' file(s)')\n",
    "\n",
    "    # Iterate through the CSV files in the input folder\n",
    "    for filename in csv_files:\n",
    "        csv_file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Append the unique rows to the DataFrame\n",
    "        unique_rows_df = pd.concat([unique_rows_df, df]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Calculate the percentage completion\n",
    "        percentage_complete = (progress_bar.n / total_files) * 100\n",
    "\n",
    "        # Print the percentage completion every 30 seconds\n",
    "        if progress_bar.n % 5 == 0:\n",
    "            print(f\"{percentage_complete:.2f}% completed\")\n",
    "\n",
    "        # Sleep for 30 seconds\n",
    "        time.sleep(30)\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Save the merged unique rows to a new CSV file\n",
    "    unique_rows_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "    print(f\"Unique rows merged and saved in '{output_csv_file}'.\")\n",
    "else:\n",
    "    print(f\"The input folder '{input_folder}' does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb  6 15:34:47 2024\n",
    "\n",
    "@author: BISHWAJIT\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(r'C:\\Unique_Unigram\\benign.csv')\n",
    "\n",
    "# Filter rows based on the allowed function prefixes and skip rows starting with \"_\"\n",
    "filtered_df = df[df['Unigram'].apply(lambda x: not x.startswith('_'))]  # Added closing parenthesis\n",
    "\n",
    "# Save the new DataFrame to a new CSV file\n",
    "filtered_df.to_csv(r'C:\\Unique_Unigram\\filtered_benign.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b0531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Dec 26 11:16:18 2023\n",
    "\n",
    "@author: Bishwajit\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#list of all CSV files\n",
    "\n",
    "file1_path = r'D:\\Feature_Vec_Creation\\Malware_Benign_Unigram'\n",
    "#unique_ngrams\n",
    "file2_path = r'D:\\Feature_Vec_Creation\\malwareplusben_cleaned.csv'\n",
    "#output csv\n",
    "output_csv_path = r'C:\\FeatureVector\\Feb_feature_benign_malware.csv'\n",
    "# Task 2: Read data from file2_path and transpose the DataFrame\n",
    "file2_df = pd.read_csv(file2_path, header=0)  # Assuming no header in file2_path\n",
    "\n",
    "# Set the first column as the index for file2_df\n",
    "file2_df.set_index(file2_df.columns[0], inplace=True)\n",
    "\n",
    "# Transpose the DataFrame and reset the index\n",
    "file2_df2 = file2_df.transpose()\n",
    "\n",
    "# Set the index of file2_df2 to be numbers from 0 to the number of rows-1\n",
    "# file2_df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Task 1: Read data from file1_path and create a dataframe\n",
    "result_df = file2_df2\n",
    "count=0\n",
    "# Iterate through all CSV files in file1_path\n",
    "for file1_file_name in os.listdir(file1_path):\n",
    "    file1_full_path = os.path.join(file1_path, file1_file_name)\n",
    "    \n",
    "    # Check if the file is empty or has rows less than or equal to 1000\n",
    "    if os.path.getsize(file1_full_path) == 0 or len(pd.read_csv(file1_full_path, skiprows=1)) <= 1000:\n",
    "        \n",
    "        print(f\"Skipping empty or small file: {file1_file_name}{count}\")\n",
    "        count=count+1\n",
    "        continue\n",
    "\n",
    "    # Extract hash and family information from the file name\n",
    "    hash_value, family_value = os.path.splitext(file1_file_name)[0].split('_')[:2]\n",
    "\n",
    "    # Create a new row for the current file in result_df\n",
    "    result_df.loc[hash_value, 'hash'] = hash_value\n",
    "    result_df.loc[hash_value, 'family'] = family_value\n",
    "\n",
    "    # Read the CSV file and compute value counts\n",
    "    file1_df = pd.read_csv(file1_full_path)\n",
    "    value_counts_dict = file1_df['Unigram'].value_counts().to_dict()\n",
    "\n",
    "    # Update result_df based on value counts for the current file\n",
    "    for key, value in value_counts_dict.items():\n",
    "        if key in result_df.columns:\n",
    "            result_df.loc[hash_value, key] = value\n",
    "\n",
    "# Fill NaN values with 0 for columns other than 'hash' and 'family'\n",
    "result_df.fillna(0, inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "result_df.reset_index(drop=True, inplace=True)\n",
    "print('Code Run Successfully')\n",
    "result_df.to_csv(output_csv_path,index=False)\n",
    "\n",
    "#############################################################################\n",
    "# Count the number of zeros in the DataFrame\n",
    "zero_count = (result_df == 0).sum().sum()\n",
    "\n",
    "# Calculate the total number of elements in the DataFrame\n",
    "total_elements = result_df.size\n",
    "\n",
    "# Calculate the percentage of zeros\n",
    "percentage_zeros = (zero_count / total_elements) * 100\n",
    "\n",
    "print(f\"Number of zeros in the DataFrame: {zero_count}\")\n",
    "print(f\"Total number of elements in the DataFrame: {total_elements}\")\n",
    "print(f\"Percentage of zeros: {percentage_zeros:.2f}%\")\n",
    "#############################################################################\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
