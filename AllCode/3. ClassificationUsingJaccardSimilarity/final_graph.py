# -*- coding: utf-8 -*-
"""
Created on Wed Jan  3 19:20:59 2024

@author: admin
"""

# -*- coding: utf-8 -*-
"""
Created on Fri Nov 10 11:09:42 2023

@author: admin
"""
#pip install prettytable
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from prettytable import PrettyTable
from PIL import Image, ImageDraw
from sklearn.metrics import roc_curve, auc, precision_recall_curve, matthews_corrcoef

# Define the class labels
classes = ['adware', 'backdoor', 'downloader', 'trojan', 'spyware', 'virus', 'worm', 'benign']

# Initialize variables to store the results
true_positive = np.zeros(len(classes))
false_positive = np.zeros(len(classes))
false_negative = np.zeros(len(classes))
true_negative = np.zeros(len(classes))
total_rows = np.zeros(len(classes))
mcc_values = np.zeros(len(classes))

# Loop through each class
for i, class_label in enumerate(classes):
    # Load the CSV file into a DataFrame
    data = pd.read_csv(r'F:\Malware\trigram_analysis\jaccard_similarity\Combined\combined_data.csv', sep=',')

    # Define the true and predicted labels for the current class
    true_labels = [1 if label == class_label else 0 for label in data['label']]
    predicted_labels = [1 if label == class_label else 0 for label in data['predicted_label']]

    # Calculate confusion matrix for the current class
    total_rows[i] = len(true_labels)
    true_positive[i] = np.sum(np.logical_and(np.array(true_labels) == 1, np.array(predicted_labels) == 1))
    false_positive[i] = np.sum(np.logical_and(np.array(true_labels) == 0, np.array(predicted_labels) == 1))
    false_negative[i] = np.sum(np.logical_and(np.array(true_labels) == 1, np.array(predicted_labels) == 0))
    true_negative[i] = np.sum(np.logical_and(np.array(true_labels) == 0, np.array(predicted_labels) == 0))
    
    # Calculate MCC for the current class
    mcc_values[i] = matthews_corrcoef(true_labels, predicted_labels)

# Calculate metrics
precision = true_positive / (true_positive + false_positive)
recall = true_positive / (true_positive + false_negative)
f1_score = 2 * (precision * recall) / (precision + recall)
accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)

# Create a table
table = PrettyTable()
table.field_names = ["Class", "Total Rows", "True Positive", "True Negative", "False Positive", "False Negative", "Precision", "Recall", "F1 Score", "Accuracy", "MCC"]

for i, class_label in enumerate(classes):
    table.add_row([
        class_label, 
        total_rows[i], 
        true_positive[i], 
        true_negative[i], 
        false_positive[i], 
        false_negative[i], 
        precision[i], 
        recall[i], 
        f1_score[i], 
        accuracy[i],
        mcc_values[i]
    ])

print(table)

# Save the table as an image
img = Image.new("RGB", (1200, 300), color="white")
d = ImageDraw.Draw(img)
d.text((10,10), table.get_string(), fill=(0,0,0,255))
img.save('table.png')

# Plot the metrics
x = np.arange(len(classes))

# Plot precision
plt.figure(figsize=(8, 6))
plt.bar(x, precision, color='red')
plt.xlabel('Classes')
plt.ylabel('Precision')
plt.title('Precision Scores')
plt.xticks(x, classes)
plt.savefig('precision.png')
plt.show()

# Plot recall
plt.figure(figsize=(8, 6))
plt.bar(x, recall, color='green')
plt.xlabel('Classes')
plt.ylabel('Recall')
plt.title('Recall Scores')
plt.xticks(x, classes)
plt.savefig('recall.png')
plt.show()

# Plot F1 score
plt.figure(figsize=(8, 6))
plt.bar(x, f1_score, color='black')
plt.xlabel('Classes')
plt.ylabel('F1 Score')
plt.title('F1 Scores')
plt.xticks(x, classes)
plt.savefig('f1_score.png')
plt.show()

# Plot accuracy
plt.figure(figsize=(8, 6))
plt.bar(x, accuracy, color='blue')
plt.xlabel('Classes')
plt.ylabel('Accuracy')
plt.title('Accuracy Scores')
plt.xticks(x, classes)
plt.savefig('accuracy.png')
plt.show()

# Plot MCC
plt.figure(figsize=(8, 6))
plt.bar(x, mcc_values, color='orange')
plt.xlabel('Classes')
plt.ylabel('MCC')
plt.title('Matthews Correlation Coefficient (MCC)')
plt.xticks(x, classes)
plt.savefig('mcc.png')
plt.show()

# Combine all metrics in one graph
plt.figure(figsize=(12, 8))
bar_width = 0.15

plt.bar(x - 2 * bar_width, precision, color='red', width=bar_width, label='Precision')
plt.bar(x - bar_width, recall, color='green', width=bar_width, label='Recall')
plt.bar(x, f1_score, color='black', width=bar_width, label='F1 Score')
plt.bar(x + bar_width, accuracy, color='blue', width=bar_width, label='Accuracy')
plt.bar(x + 2 * bar_width, mcc_values, color='orange', width=bar_width, label='MCC')

plt.xlabel('Classes')
plt.ylabel('Scores')
plt.title('Performance Metrics by Class')
plt.xticks(x, classes)
plt.legend()
plt.savefig('performance_metrics.png')
plt.show()

# Additional plots
# Calculate ROC curve and AUC for 'benign' class
data = pd.read_csv(r'F:\Malware\trigram_analysis\jaccard_similarity\Combined\combined_data.csv', sep=',')
true_labels_benign = [1 if label == 'benign' else 0 for label in data['predicted_label']]
predicted_labels_benign = [1 if label == 'benign' else 0 for label in data['label']]
fpr_benign, tpr_benign, roc_auc_benign = roc_curve(true_labels_benign, predicted_labels_benign)

# Plot ROC curve for 'benign' class
plt.figure(figsize=(8, 6))
plt.plot(fpr_benign, tpr_benign, label=f'benign (AUC = {np.mean(roc_auc_benign):0.2f})', color='purple')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve (benign class)')
plt.legend(loc='lower right')
plt.show()

# Plot Precision-Recall curve for 'benign' class
precision_benign, recall_benign, _ = precision_recall_curve(true_labels_benign, predicted_labels_benign)
plt.figure(figsize=(8, 6))
plt.plot(recall_benign, precision_benign, label='benign', color='purple')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve (benign class)')
plt.legend(loc='lower left')
plt.show()
